# Databricks notebook to read from Event Hub and write to Cosmos DB (with debug logs)

from pyspark.sql.types import StructType, StringType, DoubleType, LongType
from pyspark.sql.functions import from_json, col

print("üîç Initializing IoT telemetry pipeline...")

# Define the schema for the incoming JSON data
schema = StructType() \
    .add("deviceId", StringType()) \
    .add("latitude", DoubleType()) \
    .add("longitude", DoubleType()) \
    .add("timestamp", LongType())

# Event Hub config
ehConf = {
    'eventhubs.connectionString': var.eventhub_connection_string
}

print("üì° Event Hub configuration loaded:")
print(ehConf)

# Create or access the Spark session
try:
    spark
except NameError:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()

print("üöÄ Spark session ready. Reading from Event Hub...")

# Read from Event Hub
raw_df = spark.readStream \
    .format("eventhubs") \
    .options(**ehConf) \
    .load()

print("‚úÖ Successfully connected to Event Hub.")

# Parse Event Hub messages
json_df = raw_df.select(from_json(col("body").cast("string"), schema).alias("data")).select("data.*")
print("üß¨ Schema after parsing:")
json_df.printSchema()

# Cosmos DB config
cosmos_config = {
    "Endpoint": var.cosmos_db_endpoint,
    "Masterkey": var.cosmos_db_key,
    "Database": "LocationDB",
    "Collection": "DeviceLocations",
    "Upsert": "true"
}

print("üíæ Preparing to write to Cosmos DB...")
print("Cosmos config keys:", list(cosmos_config.keys()))

# Start the streaming job
stream = json_df.writeStream \
    .format("cosmos.oltp") \
    .options(**cosmos_config) \
    .outputMode("append") \
    .start()

print("‚úÖ Streaming pipeline initialized. Data is flowing!")